<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <title>
        O
    </title>
    
<link rel="stylesheet" href="/libs/highlight/styles/monokai-sublime.css">

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body id="bodyx">
    <div class="hd posts">
    <a href="/index.html"><i class="fa fa-reply replay-btn" aria-hidden="true"></i></a>
    <div class="post-title">
        <p>
            pytorch å­¦ä¹ 
        </p>
        <hr>
    </div>
    <div class="post-content">
        <h3 id="pytorch-å­¦ä¹ "><a href="#pytorch-å­¦ä¹ " class="headerlink" title="pytorch å­¦ä¹ "></a>pytorch å­¦ä¹ </h3><h4 id="1-pytorchçš„ä¼˜ç‚¹"><a href="#1-pytorchçš„ä¼˜ç‚¹" class="headerlink" title="1. pytorchçš„ä¼˜ç‚¹"></a>1. pytorchçš„ä¼˜ç‚¹</h4><pre><code>è‡ªåŠ¨å¾®åˆ†
é›†æˆå¸¸ç”¨å‡½æ•°
æ‹¥æœ‰æ•°æ®å¤„ç†å‡½æ•°
å¤šç²¾åº¦è®­ç»ƒ</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">466</span>)</span><br><span class="line">np.random.seed(<span class="number">466</span>)</span><br></pre></td></tr></table></figure>

<h4 id="numpy-ndarray-ä¸-pytorch-Tensors-ä¹‹é—´çš„è”ç³»"><a href="#numpy-ndarray-ä¸-pytorch-Tensors-ä¹‹é—´çš„è”ç³»" class="headerlink" title="numpy ndarray ä¸ pytorch Tensors ä¹‹é—´çš„è”ç³»"></a>numpy ndarray ä¸ pytorch Tensors ä¹‹é—´çš„è”ç³»</h4>   <a id="more"></a>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_numpy = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">x_torch = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'x_numpy,  x_torch'</span>)</span><br><span class="line">print(x_numpy,x_torch)</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>

<pre><code>x_numpy,  x_torch
[1 2 3] tensor([1, 2, 3])</code></pre><p>â€‹    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'numpy ä¸ torch ä¹‹é—´çš„è½¬åŒ–'</span>)</span><br><span class="line">print(torch.from_numpy(x_numpy),x_torch.numpy())</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>

<pre><code>numpy ä¸ torch ä¹‹é—´çš„è½¬åŒ–
tensor([1, 2, 3], dtype=torch.int32) [1 2 3]</code></pre><p>â€‹    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è¿ç®—æ“ä½œ</span></span><br><span class="line">y_numpy = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">y_torch = torch.tensor([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(<span class="string">'x+y'</span>)</span><br><span class="line">print(x_numpy+y_numpy,x_torch+y_torch)</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>

<pre><code>x+y
[3 5 7] tensor([3, 5, 7])</code></pre><p>â€‹    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"mean along the 0th dimension"</span>)</span><br><span class="line">x_numpy = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">x_torch = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">print(np.mean(x_numpy, axis=<span class="number">0</span>), torch.mean(x_torch, dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<pre><code>mean along the 0th dimension
[2. 3.] tensor([2., 3.])</code></pre><h4 id="Tensor-view-ç”¨äºreshape-tensor"><a href="#Tensor-view-ç”¨äºreshape-tensor" class="headerlink" title="Tensor.view ç”¨äºreshape tensor"></a>Tensor.view ç”¨äºreshape tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">N,C,W,H = <span class="number">10</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span></span><br><span class="line">X = torch.randn(N,C,W,H)</span><br><span class="line"></span><br><span class="line">print(X.shape)</span><br><span class="line">print(X.view(N,C,<span class="number">5</span>*<span class="number">5</span>).shape)</span><br><span class="line">print(X.view(<span class="number">-1</span>,C,<span class="number">5</span>*<span class="number">5</span>).shape)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([10, 3, 5, 5])
torch.Size([10, 3, 25])
torch.Size([10, 3, 25])</code></pre><h3 id="è®¡ç®—å›¾"><a href="#è®¡ç®—å›¾" class="headerlink" title="è®¡ç®—å›¾"></a>è®¡ç®—å›¾</h3><p>Whatâ€™s special about PyTorchâ€™s tensor object is that it implicitly creates a computation graph in the background. A computation graph is a a way of writing a mathematical expression as a graph. There is an algorithm to compute the gradients of all the variables of a computation graph in time on the same order it is to compute the function itself.</p>
<p>Consider the expression  ğ‘’=(ğ‘+ğ‘)âˆ—(ğ‘+1)  with values  ğ‘=2,ğ‘=1 . We can draw the evaluated computation graph as</p>
<p>In PyTorch, we can write this as<br><img src="/2020/05/02/pytorch_learning/tree-eval.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>,requires_grad=<span class="literal">True</span>) <span class="comment"># requires_grad è‡ªåŠ¨å¾®åˆ†å‚æ•°</span></span><br><span class="line">b = torch.tensor(<span class="number">1.0</span>,requires_grad=<span class="literal">False</span>) <span class="comment"># è¾“å…¥Falseä¸è‡ªåŠ¨è®¡ç®—è®¡ç®—å›¾</span></span><br><span class="line">c = a+b</span><br><span class="line">d = b+<span class="number">1</span></span><br><span class="line">e = c*d</span><br><span class="line">print(<span class="string">'c'</span>,c)</span><br><span class="line">print(<span class="string">'d'</span>,d)</span><br><span class="line">print(<span class="string">'e'</span>,e)</span><br></pre></td></tr></table></figure>

<pre><code>c tensor(3., grad_fn=&lt;AddBackward0&gt;)
d tensor(2.)
e tensor(6., grad_fn=&lt;MulBackward0&gt;)</code></pre><h3 id="PyTorch-çš„è‡ªåŠ¨å¾®åˆ†æ¡†æ¶"><a href="#PyTorch-çš„è‡ªåŠ¨å¾®åˆ†æ¡†æ¶" class="headerlink" title="PyTorch çš„è‡ªåŠ¨å¾®åˆ†æ¡†æ¶"></a>PyTorch çš„è‡ªåŠ¨å¾®åˆ†æ¡†æ¶</h3><p>Now that we have seen that PyTorch keeps the graph around for us, letâ€™s use it to compute some gradients for us.</p>
<p>Consider the function  $ ğ‘“(ğ‘¥)=(ğ‘¥âˆ’2)^2 $.</p>
<p>Q: Compute $\frac{d}{dx} f(x)$ and then compute $fâ€™(1)$.</p>
<p>We make a <code>backward()</code> call on the leaf variable (<code>y</code>) in the computation, computing all the gradients of <code>y</code> at once.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-2</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fp</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = f(x)</span><br><span class="line">y.backward()  <span class="comment">#backward()ç”¨äºè®¡ç®—è¡¨è¾¾å¼ä¸­çš„æ‰€æœ‰æ¢¯åº¦</span></span><br><span class="line">print(<span class="string">'PyTorch è§£ä¸ºï¼š'</span>,x.grad)</span><br><span class="line">z = f(x) * f(x**<span class="number">2</span>)</span><br><span class="line">z.backward()</span><br><span class="line">print(<span class="string">'å¾®åˆ†è§£æè§£ä¸ºï¼š'</span>,fp(x))</span><br><span class="line">print(<span class="string">'PyTorch è§£ä¸ºï¼š'</span>,x.grad)</span><br></pre></td></tr></table></figure>

<pre><code>PyTorch è§£ä¸ºï¼š tensor([-2.])
å¾®åˆ†è§£æè§£ä¸ºï¼š tensor([-2.], grad_fn=&lt;MulBackward0&gt;)
PyTorch è§£ä¸ºï¼š tensor([-8.])</code></pre><p>It can also find gradients of functions.</p>
<p>Let $w = [w_1, w_2]^T$</p>
<p>Consider $g(w) = 2w_1w_2 + w_2\cos(w_1)$</p>
<p>Q: Compute $\nabla_w g(w)$ and verify $\nabla_w g([\pi,1]) = [2, \pi - 1]^T$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*w[<span class="number">0</span>]*w[<span class="number">1</span>] + w[<span class="number">1</span>]*torch.cos(w[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor([<span class="number">2</span>*w[<span class="number">1</span>]-w[<span class="number">1</span>]*torch.sin(w[<span class="number">0</span>]),<span class="number">2</span>*w[<span class="number">0</span>]+torch.cos(w[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">w = torch.tensor([np.pi,<span class="number">1</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">z = g(w)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'è§£æè§£ï¼š'</span>, grad_g(w))</span><br><span class="line">print(<span class="string">'Torchè§£ï¼š'</span>, w.grad)</span><br></pre></td></tr></table></figure>

<pre><code>è§£æè§£ï¼š tensor([2.0000, 5.2832])
Torchè§£ï¼š tensor([2.0000, 5.2832])</code></pre><h3 id="ä½¿ç”¨æ¢¯åº¦"><a href="#ä½¿ç”¨æ¢¯åº¦" class="headerlink" title="ä½¿ç”¨æ¢¯åº¦"></a>ä½¿ç”¨æ¢¯åº¦</h3><p>Now that we have gradients, we can use our favorite optimization algorithm: gradient descent!</p>
<p>Let $f$ the same function we defined above.<br>$ f = (x-2)^2 $</p>
<p>Q: What is the value of $x$ that minimizes $f$?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.0</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">step_size = <span class="number">0.4</span></span><br><span class="line">print(<span class="string">'iter,\tx,\tf(x),\tf\'(x),\tf\'(x) pytorch'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;'</span>.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))</span><br><span class="line">    </span><br><span class="line">    x.data = x.data - step_size * x.grad</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#éœ€è¦æŠŠgradå€¼å½’é›¶ï¼Œä¸å½’é›¶çš„è¯ y çš„å€¼ä¼šç´¯ç§¯</span></span><br><span class="line">    x.grad.detach_() <span class="comment"># ä¸ºäº†æé«˜æ•ˆç‡</span></span><br><span class="line">    x.grad.zero_()</span><br></pre></td></tr></table></figure>

<pre><code>iter,    x,    f(x),    f&apos;(x),    f&apos;(x) pytorch
0,    5.000,    9.000,    6.000,    6.000
1,    2.600,    0.360,    1.200,    1.200
2,    2.120,    0.014,    0.240,    0.240
3,    2.024,    0.001,    0.048,    0.048
4,    2.005,    0.000,    0.010,    0.010
5,    2.001,    0.000,    0.002,    0.002
6,    2.000,    0.000,    0.000,    0.000
7,    2.000,    0.000,    0.000,    0.000
8,    2.000,    0.000,    0.000,    0.000
9,    2.000,    0.000,    0.000,    0.000</code></pre><h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>ä½¿ç”¨torchè¿›è¡Œçº¿æ€§å›å½’</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æ„é€ äº†ä¸€ä¸ªçº¿æ€§å¸¦æœ‰å™ªéŸ³çš„data set</span></span><br><span class="line">d = <span class="number">2</span></span><br><span class="line">n = <span class="number">50</span></span><br><span class="line">X = torch.randn(n,d)</span><br><span class="line">true_w = torch.tensor([[<span class="number">-1.0</span>],[<span class="number">2.0</span>]])</span><br><span class="line">y = X @ true_w + torch.randn(n,<span class="number">1</span>)*<span class="number">0.1</span></span><br><span class="line">print(<span class="string">'X shape'</span>,X.shape)</span><br><span class="line">print(<span class="string">'y shape'</span>, y.shape)</span><br><span class="line">print(<span class="string">'w shape'</span>, true_w.shape)</span><br></pre></td></tr></table></figure>

<pre><code>X shape torch.Size([50, 2])
y shape torch.Size([50, 1])
w shape torch.Size([2, 1])</code></pre><h3 id="Note-dimensions"><a href="#Note-dimensions" class="headerlink" title="Note: dimensions"></a>Note: dimensions</h3><p>PyTorch does a lot of operations on batches of data. The convention is to have your data be of size $(N, d)$ where $N$ is the size of the batch of data.</p>
<h3 id="Sanity-check"><a href="#Sanity-check" class="headerlink" title="Sanity check"></a>Sanity check</h3><p>To verify PyTorch is computing the gradients correctly, letâ€™s recall the gradient for the RSS objective:</p>
<p>$$\nabla_w \mathcal{L}_{RSS}(w; X) = \nabla_w\frac{1}{n} ||y - Xw||_2^2 = -\frac{2}{n}X^T(y-Xw)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X,w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> X @ w</span><br><span class="line"></span><br><span class="line"><span class="comment"># norm æ±‚çš„æ˜¯çŸ©é˜µèŒƒæ•°</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rss</span><span class="params">(y,y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(y-y_hat)**<span class="number">2</span>/n</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¢¯åº¦è§£æè§£ @ è¡¨ç¤ºçŸ©é˜µä¹˜æ³•</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_rss</span><span class="params">(X,y,W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>*X.t()@(y-X @ w)/n</span><br><span class="line"></span><br><span class="line">w = torch.tensor([[<span class="number">1.0</span>],[<span class="number">0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_hat = model(X,w)</span><br><span class="line"></span><br><span class="line">loss = rss(y,y_hat)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical gradient'</span>, grad_rss(X, y, w).detach().view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'PyTorch\'s gradient'</span>, w.grad.view(<span class="number">2</span>).numpy())</span><br></pre></td></tr></table></figure>

<pre><code>Analytical gradient [ 4.474982 -3.335253]
PyTorch&apos;s gradient [ 4.4749813 -3.3352537]</code></pre><p>Now that weâ€™ve seen PyTorch is doing the right think, letâ€™s use the gradients!</p>
<h2 id="Linear-regression-using-GD-with-automatically-computed-derivatives"><a href="#Linear-regression-using-GD-with-automatically-computed-derivatives" class="headerlink" title="Linear regression using GD with automatically computed derivatives"></a>Linear regression using GD with automatically computed derivatives</h2><p>We will now use the gradients to run the gradient descent algorithm.</p>
<p>Note: This example is an illustration to connect ideas we have seen before to PyTorchâ€™s way of doing things. We will see how to do this in the â€œPyTorchicâ€ way in the next example.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">step_size = <span class="number">0.4</span></span><br><span class="line">print(<span class="string">'iter,\t loss,\t w'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = model(X,w)</span><br><span class="line">    loss = rss(y,y_hat)</span><br><span class="line">    </span><br><span class="line">    loss.backward()</span><br><span class="line">    w.data = w.data - step_size * w.grad</span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), w.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    w.grad.detach()</span><br><span class="line">    w.grad.zero_()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, w.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<pre><code>iter,     loss,     w
0,    7.80,    [-2.5799851  2.668203 ]
1,    3.22,    [-1.1622744  2.2072678]
2,    0.08,    [-0.997594   2.0795758]
3,    0.02,    [-0.98061234  2.0375962 ]
4,    0.01,    [-0.97969574  2.022719  ]
5,    0.01,    [-0.9800115  2.0172987]
6,    0.01,    [-0.98020816  2.0153053 ]
7,    0.01,    [-0.98029053  2.0145698 ]
8,    0.01,    [-0.98032224  2.014298  ]
9,    0.01,    [-0.9803341  2.0141976]
10,    0.01,    [-0.98033845  2.0141604 ]
11,    0.01,    [-0.98034006  2.0141468 ]
12,    0.01,    [-0.98034066  2.0141418 ]
13,    0.01,    [-0.9803409  2.01414  ]
14,    0.01,    [-0.98034096  2.0141392 ]
15,    0.01,    [-0.980341  2.014139]
16,    0.01,    [-0.980341  2.014139]
17,    0.01,    [-0.980341  2.014139]
18,    0.01,    [-0.980341  2.014139]
19,    0.01,    [-0.980341  2.014139]

true w         [-1.  2.]
estimated w     [-0.980341  2.014139]</code></pre><h3 id="Neural-Network-Basics-in-PyTorch"><a href="#Neural-Network-Basics-in-PyTorch" class="headerlink" title="Neural Network Basics in PyTorch"></a>Neural Network Basics in PyTorch</h3><p>fitting a simple neural network to the data </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">d = <span class="number">1</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line">X = torch.rand(n,d)</span><br><span class="line">y = <span class="number">4</span>* torch.sin(np.pi*X)*torch.cos(<span class="number">6</span>*np.pi*X**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X.numpy(),y.numpy())</span><br><span class="line">plt.title(<span class="string">'plot of $f(x)$'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$x$'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$y$'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/05/02/pytorch_learning/output_25_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">step_size = <span class="number">0.05</span></span><br><span class="line">iter_times = <span class="number">12000</span></span><br><span class="line">n_hidden_1 = <span class="number">32</span></span><br><span class="line">n_hidden_2 = <span class="number">32</span></span><br><span class="line">d_out = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">neural_network = nn.Sequential(</span><br><span class="line">                                nn.Linear(d,n_hidden_1),</span><br><span class="line">                                nn.Tanh(),</span><br><span class="line">                                nn.Linear(n_hidden_1,n_hidden_2),</span><br><span class="line">                                nn.Tanh(),</span><br><span class="line">                                nn.Linear(n_hidden_2,d_out))</span><br><span class="line"></span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">optim = torch.optim.SGD(neural_network.parameters(),lr=step_size)</span><br><span class="line">print(<span class="string">'iter, \t loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iter_times):</span><br><span class="line">    y_hat = neural_network(X)</span><br><span class="line">    loss = loss_func(y_hat,y)</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">1000</span> ==<span class="number">0</span> :</span><br><span class="line">        print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;'</span>.format(i, loss.item()))</span><br></pre></td></tr></table></figure>

<pre><code>iter,      loss
0,    4.36
1000,    2.60
2000,    1.08
3000,    0.62
4000,    0.23
5000,    0.12
6000,    0.06
7000,    0.05
8000,    0.03
9000,    0.02
10000,    0.00
11000,    0.01</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X_grid = torch.from_numpy(np.linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">50</span>)).float().view(<span class="number">-1</span>,d)</span><br><span class="line">y_hat = neural_network(X_grid)</span><br><span class="line">plt.scatter(X.numpy(), y.numpy())</span><br><span class="line">plt.plot(X_grid.detach().numpy(), y_hat.detach().numpy(), <span class="string">'r'</span>)</span><br><span class="line">plt.title(<span class="string">'plot of $f(x)$ and $\hat&#123;f&#125;(x)$'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$x$'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$y$'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/05/02/pytorch_learning/output_27_0.png" alt="png"></p>

    </div>

    
</div>
    <div class="footer" id="footer">
    <p>Copyright Â© 2020 <a class="flink" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>-<a class="flink" href="https://github.com/sanjinhub/hexo-theme-geek" target="_blank" rel="noopener">Geek</a>.
        <label class="el-switch el-switch-green el-switch-sm" style="vertical-align: sub;">
            <input type="checkbox" name="switch" id="update_style">
            <span class="el-switch-style"></span>
        </label>
<!--         <script type="text/javascript">
        var cnzz_protocol = (("https:" == document.location.protocol) ? "https://" : "http://");
        document.write(unescape("%3Cspan id='cnzz_stat_icon_1278548644'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/stat.php%3Fid%3D1278548644%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
        </script> -->
    </p>
</div>
<input type="hidden" id="web_style" value="black">
<input type="hidden" id="valine_appid" value="CmCti21ooOOIzFOhEyFkFvR0-gzGzoHsz">
<input type="hidden" id="valine_appKey" value="FqiyUqbg7McKN2eG0MCewupf">

<script src="/libs/jquery.min.js"></script>


<script src="/libs/highlight/highlight.pack.js"></script>

<script src='//cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>

<script src="/js/js.js"></script>

<style type="text/css">
.v * {
    color: #698fca;
}

.v .vlist .vcard .vhead .vsys {
    color: #3a3e4a;
}

.v .vlist .vcard .vh .vmeta .vat {
    color: #638fd5;
}

.v .vlist .vcard .vhead .vnick {
    color: #6ba1ff;
}

.v a {
    color: #8696b1;
}

.v .vlist .vcard .vhead .vnick:hover {
    color: #669bfc;
}
</style>
</body>

</html>
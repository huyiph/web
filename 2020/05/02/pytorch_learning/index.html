<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <title>
        O
    </title>
    
<link rel="stylesheet" href="/libs/highlight/styles/monokai-sublime.css">

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body id="bodyx">
    <div class="hd posts">
    <a href="/index.html"><i class="fa fa-reply replay-btn" aria-hidden="true"></i></a>
    <div class="post-title">
        <p>
            pytorch 学习
        </p>
        <hr>
    </div>
    <div class="post-content">
        <h3 id="pytorch-学习"><a href="#pytorch-学习" class="headerlink" title="pytorch 学习"></a>pytorch 学习</h3><h4 id="1-pytorch的优点"><a href="#1-pytorch的优点" class="headerlink" title="1. pytorch的优点"></a>1. pytorch的优点</h4><pre><code>自动微分
集成常用函数
拥有数据处理函数
多精度训练</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">466</span>)</span><br><span class="line">np.random.seed(<span class="number">466</span>)</span><br></pre></td></tr></table></figure>

<h4 id="numpy-ndarray-与-pytorch-Tensors-之间的联系"><a href="#numpy-ndarray-与-pytorch-Tensors-之间的联系" class="headerlink" title="numpy ndarray 与 pytorch Tensors 之间的联系"></a>numpy ndarray 与 pytorch Tensors 之间的联系</h4>   <a id="more"></a>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_numpy = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">x_torch = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">print(<span class="string">'x_numpy,  x_torch'</span>)</span><br><span class="line">print(x_numpy,x_torch)</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>

<pre><code>x_numpy,  x_torch
[1 2 3] tensor([1, 2, 3])</code></pre><p>​    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'numpy 与 torch 之间的转化'</span>)</span><br><span class="line">print(torch.from_numpy(x_numpy),x_torch.numpy())</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>

<pre><code>numpy 与 torch 之间的转化
tensor([1, 2, 3], dtype=torch.int32) [1 2 3]</code></pre><p>​    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运算操作</span></span><br><span class="line">y_numpy = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">y_torch = torch.tensor([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(<span class="string">'x+y'</span>)</span><br><span class="line">print(x_numpy+y_numpy,x_torch+y_torch)</span><br><span class="line">print()</span><br></pre></td></tr></table></figure>

<pre><code>x+y
[3 5 7] tensor([3, 5, 7])</code></pre><p>​    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"mean along the 0th dimension"</span>)</span><br><span class="line">x_numpy = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">x_torch = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4.</span>]])</span><br><span class="line">print(np.mean(x_numpy, axis=<span class="number">0</span>), torch.mean(x_torch, dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<pre><code>mean along the 0th dimension
[2. 3.] tensor([2., 3.])</code></pre><h4 id="Tensor-view-用于reshape-tensor"><a href="#Tensor-view-用于reshape-tensor" class="headerlink" title="Tensor.view 用于reshape tensor"></a>Tensor.view 用于reshape tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">N,C,W,H = <span class="number">10</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span></span><br><span class="line">X = torch.randn(N,C,W,H)</span><br><span class="line"></span><br><span class="line">print(X.shape)</span><br><span class="line">print(X.view(N,C,<span class="number">5</span>*<span class="number">5</span>).shape)</span><br><span class="line">print(X.view(<span class="number">-1</span>,C,<span class="number">5</span>*<span class="number">5</span>).shape)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([10, 3, 5, 5])
torch.Size([10, 3, 25])
torch.Size([10, 3, 25])</code></pre><h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>What’s special about PyTorch’s tensor object is that it implicitly creates a computation graph in the background. A computation graph is a a way of writing a mathematical expression as a graph. There is an algorithm to compute the gradients of all the variables of a computation graph in time on the same order it is to compute the function itself.</p>
<p>Consider the expression  𝑒=(𝑎+𝑏)∗(𝑏+1)  with values  𝑎=2,𝑏=1 . We can draw the evaluated computation graph as</p>
<p>In PyTorch, we can write this as<br><img src="/2020/05/02/pytorch_learning/tree-eval.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>,requires_grad=<span class="literal">True</span>) <span class="comment"># requires_grad 自动微分参数</span></span><br><span class="line">b = torch.tensor(<span class="number">1.0</span>,requires_grad=<span class="literal">False</span>) <span class="comment"># 输入False不自动计算计算图</span></span><br><span class="line">c = a+b</span><br><span class="line">d = b+<span class="number">1</span></span><br><span class="line">e = c*d</span><br><span class="line">print(<span class="string">'c'</span>,c)</span><br><span class="line">print(<span class="string">'d'</span>,d)</span><br><span class="line">print(<span class="string">'e'</span>,e)</span><br></pre></td></tr></table></figure>

<pre><code>c tensor(3., grad_fn=&lt;AddBackward0&gt;)
d tensor(2.)
e tensor(6., grad_fn=&lt;MulBackward0&gt;)</code></pre><h3 id="PyTorch-的自动微分框架"><a href="#PyTorch-的自动微分框架" class="headerlink" title="PyTorch 的自动微分框架"></a>PyTorch 的自动微分框架</h3><p>Now that we have seen that PyTorch keeps the graph around for us, let’s use it to compute some gradients for us.</p>
<p>Consider the function  $ 𝑓(𝑥)=(𝑥−2)^2 $.</p>
<p>Q: Compute $\frac{d}{dx} f(x)$ and then compute $f’(1)$.</p>
<p>We make a <code>backward()</code> call on the leaf variable (<code>y</code>) in the computation, computing all the gradients of <code>y</code> at once.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-2</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fp</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = f(x)</span><br><span class="line">y.backward()  <span class="comment">#backward()用于计算表达式中的所有梯度</span></span><br><span class="line">print(<span class="string">'PyTorch 解为：'</span>,x.grad)</span><br><span class="line">z = f(x) * f(x**<span class="number">2</span>)</span><br><span class="line">z.backward()</span><br><span class="line">print(<span class="string">'微分解析解为：'</span>,fp(x))</span><br><span class="line">print(<span class="string">'PyTorch 解为：'</span>,x.grad)</span><br></pre></td></tr></table></figure>

<pre><code>PyTorch 解为： tensor([-2.])
微分解析解为： tensor([-2.], grad_fn=&lt;MulBackward0&gt;)
PyTorch 解为： tensor([-8.])</code></pre><p>It can also find gradients of functions.</p>
<p>Let $w = [w_1, w_2]^T$</p>
<p>Consider $g(w) = 2w_1w_2 + w_2\cos(w_1)$</p>
<p>Q: Compute $\nabla_w g(w)$ and verify $\nabla_w g([\pi,1]) = [2, \pi - 1]^T$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*w[<span class="number">0</span>]*w[<span class="number">1</span>] + w[<span class="number">1</span>]*torch.cos(w[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_g</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor([<span class="number">2</span>*w[<span class="number">1</span>]-w[<span class="number">1</span>]*torch.sin(w[<span class="number">0</span>]),<span class="number">2</span>*w[<span class="number">0</span>]+torch.cos(w[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">w = torch.tensor([np.pi,<span class="number">1</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">z = g(w)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'解析解：'</span>, grad_g(w))</span><br><span class="line">print(<span class="string">'Torch解：'</span>, w.grad)</span><br></pre></td></tr></table></figure>

<pre><code>解析解： tensor([2.0000, 5.2832])
Torch解： tensor([2.0000, 5.2832])</code></pre><h3 id="使用梯度"><a href="#使用梯度" class="headerlink" title="使用梯度"></a>使用梯度</h3><p>Now that we have gradients, we can use our favorite optimization algorithm: gradient descent!</p>
<p>Let $f$ the same function we defined above.<br>$ f = (x-2)^2 $</p>
<p>Q: What is the value of $x$ that minimizes $f$?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.0</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">step_size = <span class="number">0.4</span></span><br><span class="line">print(<span class="string">'iter,\tx,\tf(x),\tf\'(x),\tf\'(x) pytorch'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;,\t&#123;:.3f&#125;'</span>.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))</span><br><span class="line">    </span><br><span class="line">    x.data = x.data - step_size * x.grad</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#需要把grad值归零，不归零的话 y 的值会累积</span></span><br><span class="line">    x.grad.detach_() <span class="comment"># 为了提高效率</span></span><br><span class="line">    x.grad.zero_()</span><br></pre></td></tr></table></figure>

<pre><code>iter,    x,    f(x),    f&apos;(x),    f&apos;(x) pytorch
0,    5.000,    9.000,    6.000,    6.000
1,    2.600,    0.360,    1.200,    1.200
2,    2.120,    0.014,    0.240,    0.240
3,    2.024,    0.001,    0.048,    0.048
4,    2.005,    0.000,    0.010,    0.010
5,    2.001,    0.000,    0.002,    0.002
6,    2.000,    0.000,    0.000,    0.000
7,    2.000,    0.000,    0.000,    0.000
8,    2.000,    0.000,    0.000,    0.000
9,    2.000,    0.000,    0.000,    0.000</code></pre><h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>使用torch进行线性回归</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造了一个线性带有噪音的data set</span></span><br><span class="line">d = <span class="number">2</span></span><br><span class="line">n = <span class="number">50</span></span><br><span class="line">X = torch.randn(n,d)</span><br><span class="line">true_w = torch.tensor([[<span class="number">-1.0</span>],[<span class="number">2.0</span>]])</span><br><span class="line">y = X @ true_w + torch.randn(n,<span class="number">1</span>)*<span class="number">0.1</span></span><br><span class="line">print(<span class="string">'X shape'</span>,X.shape)</span><br><span class="line">print(<span class="string">'y shape'</span>, y.shape)</span><br><span class="line">print(<span class="string">'w shape'</span>, true_w.shape)</span><br></pre></td></tr></table></figure>

<pre><code>X shape torch.Size([50, 2])
y shape torch.Size([50, 1])
w shape torch.Size([2, 1])</code></pre><h3 id="Note-dimensions"><a href="#Note-dimensions" class="headerlink" title="Note: dimensions"></a>Note: dimensions</h3><p>PyTorch does a lot of operations on batches of data. The convention is to have your data be of size $(N, d)$ where $N$ is the size of the batch of data.</p>
<h3 id="Sanity-check"><a href="#Sanity-check" class="headerlink" title="Sanity check"></a>Sanity check</h3><p>To verify PyTorch is computing the gradients correctly, let’s recall the gradient for the RSS objective:</p>
<p>$$\nabla_w \mathcal{L}_{RSS}(w; X) = \nabla_w\frac{1}{n} ||y - Xw||_2^2 = -\frac{2}{n}X^T(y-Xw)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X,w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> X @ w</span><br><span class="line"></span><br><span class="line"><span class="comment"># norm 求的是矩阵范数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rss</span><span class="params">(y,y_hat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.norm(y-y_hat)**<span class="number">2</span>/n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度解析解 @ 表示矩阵乘法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_rss</span><span class="params">(X,y,W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>*X.t()@(y-X @ w)/n</span><br><span class="line"></span><br><span class="line">w = torch.tensor([[<span class="number">1.0</span>],[<span class="number">0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_hat = model(X,w)</span><br><span class="line"></span><br><span class="line">loss = rss(y,y_hat)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Analytical gradient'</span>, grad_rss(X, y, w).detach().view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'PyTorch\'s gradient'</span>, w.grad.view(<span class="number">2</span>).numpy())</span><br></pre></td></tr></table></figure>

<pre><code>Analytical gradient [ 4.474982 -3.335253]
PyTorch&apos;s gradient [ 4.4749813 -3.3352537]</code></pre><p>Now that we’ve seen PyTorch is doing the right think, let’s use the gradients!</p>
<h2 id="Linear-regression-using-GD-with-automatically-computed-derivatives"><a href="#Linear-regression-using-GD-with-automatically-computed-derivatives" class="headerlink" title="Linear regression using GD with automatically computed derivatives"></a>Linear regression using GD with automatically computed derivatives</h2><p>We will now use the gradients to run the gradient descent algorithm.</p>
<p>Note: This example is an illustration to connect ideas we have seen before to PyTorch’s way of doing things. We will see how to do this in the “PyTorchic” way in the next example.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">step_size = <span class="number">0.4</span></span><br><span class="line">print(<span class="string">'iter,\t loss,\t w'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_hat = model(X,w)</span><br><span class="line">    loss = rss(y,y_hat)</span><br><span class="line">    </span><br><span class="line">    loss.backward()</span><br><span class="line">    w.data = w.data - step_size * w.grad</span><br><span class="line">    print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;,\t&#123;&#125;'</span>.format(i, loss.item(), w.view(<span class="number">2</span>).detach().numpy()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We need to zero the grad variable since the backward()</span></span><br><span class="line">    <span class="comment"># call accumulates the gradients in .grad instead of overwriting.</span></span><br><span class="line">    <span class="comment"># The detach_() is for efficiency. You do not need to worry too much about it.</span></span><br><span class="line">    w.grad.detach()</span><br><span class="line">    w.grad.zero_()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\ntrue w\t\t'</span>, true_w.view(<span class="number">2</span>).numpy())</span><br><span class="line">print(<span class="string">'estimated w\t'</span>, w.view(<span class="number">2</span>).detach().numpy())</span><br></pre></td></tr></table></figure>

<pre><code>iter,     loss,     w
0,    7.80,    [-2.5799851  2.668203 ]
1,    3.22,    [-1.1622744  2.2072678]
2,    0.08,    [-0.997594   2.0795758]
3,    0.02,    [-0.98061234  2.0375962 ]
4,    0.01,    [-0.97969574  2.022719  ]
5,    0.01,    [-0.9800115  2.0172987]
6,    0.01,    [-0.98020816  2.0153053 ]
7,    0.01,    [-0.98029053  2.0145698 ]
8,    0.01,    [-0.98032224  2.014298  ]
9,    0.01,    [-0.9803341  2.0141976]
10,    0.01,    [-0.98033845  2.0141604 ]
11,    0.01,    [-0.98034006  2.0141468 ]
12,    0.01,    [-0.98034066  2.0141418 ]
13,    0.01,    [-0.9803409  2.01414  ]
14,    0.01,    [-0.98034096  2.0141392 ]
15,    0.01,    [-0.980341  2.014139]
16,    0.01,    [-0.980341  2.014139]
17,    0.01,    [-0.980341  2.014139]
18,    0.01,    [-0.980341  2.014139]
19,    0.01,    [-0.980341  2.014139]

true w         [-1.  2.]
estimated w     [-0.980341  2.014139]</code></pre><h3 id="Neural-Network-Basics-in-PyTorch"><a href="#Neural-Network-Basics-in-PyTorch" class="headerlink" title="Neural Network Basics in PyTorch"></a>Neural Network Basics in PyTorch</h3><p>fitting a simple neural network to the data </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">d = <span class="number">1</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line">X = torch.rand(n,d)</span><br><span class="line">y = <span class="number">4</span>* torch.sin(np.pi*X)*torch.cos(<span class="number">6</span>*np.pi*X**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X.numpy(),y.numpy())</span><br><span class="line">plt.title(<span class="string">'plot of $f(x)$'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$x$'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$y$'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/05/02/pytorch_learning/output_25_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">step_size = <span class="number">0.05</span></span><br><span class="line">iter_times = <span class="number">12000</span></span><br><span class="line">n_hidden_1 = <span class="number">32</span></span><br><span class="line">n_hidden_2 = <span class="number">32</span></span><br><span class="line">d_out = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">neural_network = nn.Sequential(</span><br><span class="line">                                nn.Linear(d,n_hidden_1),</span><br><span class="line">                                nn.Tanh(),</span><br><span class="line">                                nn.Linear(n_hidden_1,n_hidden_2),</span><br><span class="line">                                nn.Tanh(),</span><br><span class="line">                                nn.Linear(n_hidden_2,d_out))</span><br><span class="line"></span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">optim = torch.optim.SGD(neural_network.parameters(),lr=step_size)</span><br><span class="line">print(<span class="string">'iter, \t loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iter_times):</span><br><span class="line">    y_hat = neural_network(X)</span><br><span class="line">    loss = loss_func(y_hat,y)</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">1000</span> ==<span class="number">0</span> :</span><br><span class="line">        print(<span class="string">'&#123;&#125;,\t&#123;:.2f&#125;'</span>.format(i, loss.item()))</span><br></pre></td></tr></table></figure>

<pre><code>iter,      loss
0,    4.36
1000,    2.60
2000,    1.08
3000,    0.62
4000,    0.23
5000,    0.12
6000,    0.06
7000,    0.05
8000,    0.03
9000,    0.02
10000,    0.00
11000,    0.01</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X_grid = torch.from_numpy(np.linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">50</span>)).float().view(<span class="number">-1</span>,d)</span><br><span class="line">y_hat = neural_network(X_grid)</span><br><span class="line">plt.scatter(X.numpy(), y.numpy())</span><br><span class="line">plt.plot(X_grid.detach().numpy(), y_hat.detach().numpy(), <span class="string">'r'</span>)</span><br><span class="line">plt.title(<span class="string">'plot of $f(x)$ and $\hat&#123;f&#125;(x)$'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$x$'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$y$'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/05/02/pytorch_learning/output_27_0.png" alt="png"></p>

    </div>

    
</div>
    <div class="footer" id="footer">
    <p>Copyright © 2020 <a class="flink" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>-<a class="flink" href="https://github.com/sanjinhub/hexo-theme-geek" target="_blank" rel="noopener">Geek</a>.
        <label class="el-switch el-switch-green el-switch-sm" style="vertical-align: sub;">
            <input type="checkbox" name="switch" id="update_style">
            <span class="el-switch-style"></span>
        </label>
<!--         <script type="text/javascript">
        var cnzz_protocol = (("https:" == document.location.protocol) ? "https://" : "http://");
        document.write(unescape("%3Cspan id='cnzz_stat_icon_1278548644'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/stat.php%3Fid%3D1278548644%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
        </script> -->
    </p>
</div>
<input type="hidden" id="web_style" value="black">
<input type="hidden" id="valine_appid" value="CmCti21ooOOIzFOhEyFkFvR0-gzGzoHsz">
<input type="hidden" id="valine_appKey" value="FqiyUqbg7McKN2eG0MCewupf">

<script src="/libs/jquery.min.js"></script>


<script src="/libs/highlight/highlight.pack.js"></script>

<script src='//cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>

<script src="/js/js.js"></script>

<style type="text/css">
.v * {
    color: #698fca;
}

.v .vlist .vcard .vhead .vsys {
    color: #3a3e4a;
}

.v .vlist .vcard .vh .vmeta .vat {
    color: #638fd5;
}

.v .vlist .vcard .vhead .vnick {
    color: #6ba1ff;
}

.v a {
    color: #8696b1;
}

.v .vlist .vcard .vhead .vnick:hover {
    color: #669bfc;
}
</style>
</body>

</html>